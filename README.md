<img width="1680" alt="Screenshot 2025-05-05 at 11 23 25 PM" src="https://github.com/user-attachments/assets/b96d09eb-a412-43d8-8656-e3fbc0bba117" />
<img width="1680" alt="Screenshot 2025-05-05 at 10 08 16 PM" src="https://github.com/user-attachments/assets/868692b2-1fb3-4683-81d7-83ad058105ff" />
![detailed](https://github.com/user-attachments/assets/71ae354e-7bbe-4ae8-b1b5-c43dd2c0f103)
![sample2-graph](https://github.com/user-attachments/assets/a284106e-e15c-4a31-a02b-da206d91293a)
![Sample1-graph](https://github.com/user-attachments/assets/caaa51be-db77-424c-ad56-5be5d548a9cc)
![LLM generated Code Inefficiencies](https://github.com/user-attachments/assets/70f7d5a0-e34e-4be6-91fb-058dbe881cf4)
![hard-problem](https://github.com/user-attachments/assets/955e9ad7-9176-4167-9224-b5f0776906d6)
# LLM Code Efficiency Benchmark

The Efficiency of LLM-Generated Code for Binary Search: A Comparative Study

🧠 Abstract: 
This project evaluates the execution efficiency and memory usage of binary search code generated by three large language models (LLMs)—Gemini 2.0, Llama 4 Scout, and Mistral Small 3.1—compared to human-optimized code. Using the EffiBench dataset (30 problems: 10 easy, 10 medium, 10 hard), we measure execution time and memory metrics to analyze trade-offs. Our findings highlight the current limitations of LLM-generated code in performance-critical contexts.

📚 Background
Binary search is an ideal benchmark due to its widespread use and sensitivity to implementation efficiency. LLMs can quickly generate functionally correct code, but their solutions may be suboptimal in resource usage.

🛠️ Methodology
Dataset: EffiBench (30 problems)

Models Evaluated: Gemini 2.0, Llama 4 Scout, Mistral Small 3.1

Metrics:

Execution Time (ET), Normalized ET

Max Memory Usage (MU), Normalized MU

Total Memory Usage (TMU), Normalized TMU

Tools: Custom benchmarking scripts, Python, data visualization (bar/box plots)

📊 Results Summary
Human-optimized code outperformed all LLMs in execution time and memory.

LLMs varied in efficiency, especially for hard problems.

Mistral Small 3.1 showed faster execution on easy tasks but struggled with memory usage.

📈 Visualizations
Average Execution Time by Difficulty

Memory Usage Comparison

Hard Problem Performance Charts

🔍 Key Findings
LLMs provide fast code generation but sacrifice runtime and memory efficiency.

Human expertise remains essential for optimizing algorithmic code.

Hybrid approaches (LLM + human refinement) may offer the best of both worlds.

📁 File Structure
bash
Copy
Edit
├── data/                 # Raw and processed benchmark data
├── code/                 # Binary search implementations (Human & LLM)
├── visualizations/       # Graphs and plots
├── report/               # Final report (PDF/DOCX)
└── README.md             # Project overview
