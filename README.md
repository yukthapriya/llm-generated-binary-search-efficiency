
# LLM Code Efficiency Benchmark

The Efficiency of LLM-Generated Code for Binary Search: A Comparative Study

üß† Abstract: 
This project evaluates the execution efficiency and memory usage of binary search code generated by three large language models (LLMs)‚ÄîGemini 2.0, Llama 4 Scout, and Mistral Small 3.1‚Äîcompared to human-optimized code. Using the EffiBench dataset (30 problems: 10 easy, 10 medium, 10 hard), we measure execution time and memory metrics to analyze trade-offs. Our findings highlight the current limitations of LLM-generated code in performance-critical contexts.

üìö Background
Binary search is an ideal benchmark due to its widespread use and sensitivity to implementation efficiency. LLMs can quickly generate functionally correct code, but their solutions may be suboptimal in resource usage.

üõ†Ô∏è Methodology
Dataset: EffiBench (30 problems)

Models Evaluated: Gemini 2.0, Llama 4 Scout, Mistral Small 3.1

Metrics:

Execution Time (ET), Normalized ET

Max Memory Usage (MU), Normalized MU

Total Memory Usage (TMU), Normalized TMU

Tools: Custom benchmarking scripts, Python, data visualization (bar/box plots)

üìä Results Summary
Human-optimized code outperformed all LLMs in execution time and memory.

LLMs varied in efficiency, especially for hard problems.

Mistral Small 3.1 showed faster execution on easy tasks but struggled with memory usage.
